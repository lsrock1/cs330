{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CS330_Homework1_Stencil.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvkoC8rAYBE7"
      },
      "source": [
        "\n",
        "##Setup\n",
        "\n",
        "You will need to make a copy of this Colab notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q4lGYC_E6QQ",
        "outputId": "4ecf7518-73f9-4997-caa8-d73ca314988a"
      },
      "source": [
        "import os\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "# Need to download the Omniglot dataset -- DON'T MODIFY THIS CELL\n",
        "if not os.path.isdir('./omniglot_resized'):\n",
        "    gdd.download_file_from_google_drive(file_id='1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI',\n",
        "                                        dest_path='./omniglot_resized.zip',\n",
        "                                        unzip=True)\n",
        "    \n",
        "assert os.path.isdir('./omniglot_resized')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI into ./omniglot_resized.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucYsULp9HUJy"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from scipy import misc\n",
        "\n",
        "\n",
        "def get_images(paths, labels, nb_samples=None, shuffle=True):\n",
        "    \"\"\"\n",
        "    Takes a set of character folders and labels and returns paths to image files\n",
        "    paired with labels.\n",
        "    Args:\n",
        "        paths: A list of character folders\n",
        "        labels: List or numpy array of same length as paths\n",
        "        nb_samples: Number of images to retrieve per character\n",
        "    Returns:\n",
        "        List of (label, image_path) tuples\n",
        "    \"\"\"\n",
        "    if nb_samples is not None:\n",
        "        sampler = lambda x: random.sample(x, nb_samples)\n",
        "    else:\n",
        "        sampler = lambda x: x\n",
        "\n",
        "    images_labels = [(i, os.path.join(path, image.decode('UTF-8')))\n",
        "                     for i, path in zip(labels, paths)\n",
        "                     for image in sampler(os.listdir(path))]\n",
        "    if shuffle:\n",
        "        random.shuffle(images_labels)\n",
        "    return images_labels\n",
        "\n",
        "\n",
        "def image_file_to_array(filename, dim_input=None):\n",
        "    \"\"\"\n",
        "    Takes an image path and returns numpy array\n",
        "    Args:\n",
        "        filename: Image filename\n",
        "        dim_input: Flattened shape of image\n",
        "    Returns:\n",
        "        1 channel image\n",
        "    \"\"\"\n",
        "    import imageio\n",
        "    image = imageio.imread(filename)  # misc.imread(filename)\n",
        "    image = image.reshape([-1])\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "    image = 1.0 - image\n",
        "    return image\n",
        "\n",
        "\n",
        "def image_files_to_array(filenames):\n",
        "    # files, dimension\n",
        "    return np.stack([image_file_to_array(filename) for filename in filenames], axis=0)\n",
        "\n",
        "\n",
        "class DataGenerator(object):\n",
        "    \"\"\"\n",
        "    Data Generator capable of generating batches of Omniglot data.\n",
        "    A \"class\" is considered a class of omniglot digits.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, num_samples_per_class, config={}):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_classes: Number of classes for classification (K-way)\n",
        "            num_samples_per_class: num samples to generate per class in one batch\n",
        "            batch_size: size of meta batch size (e.g. number of functions)\n",
        "        \"\"\"\n",
        "        self.num_samples_per_class = num_samples_per_class\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        data_folder = config.get('data_folder', './omniglot_resized')\n",
        "        self.img_size = config.get('img_size', (28, 28))\n",
        "\n",
        "        self.dim_input = np.prod(self.img_size)\n",
        "        self.dim_output = self.num_classes\n",
        "\n",
        "        character_folders = [os.path.join(data_folder, family, character)\n",
        "                             for family in os.listdir(data_folder)\n",
        "                             if os.path.isdir(os.path.join(data_folder, family))\n",
        "                             for character in os.listdir(os.path.join(data_folder, family))\n",
        "                             if os.path.isdir(os.path.join(data_folder, family, character))]\n",
        "\n",
        "        random.seed(1)\n",
        "        random.shuffle(character_folders)\n",
        "        num_val = 100\n",
        "        num_train = 1100\n",
        "        self.metatrain_character_folders = character_folders[: num_train]\n",
        "        self.metaval_character_folders = character_folders[\n",
        "            num_train:num_train + num_val]\n",
        "        self.metatest_character_folders = character_folders[\n",
        "            num_train + num_val:]\n",
        "\n",
        "    def sample_batch(self, batch_type, batch_size):\n",
        "        \"\"\"\n",
        "        Samples a batch for training, validation, or testing\n",
        "        Args:\n",
        "            batch_type: train/val/test\n",
        "        Returns:\n",
        "            A a tuple of (1) Image batch and (2) Label batch where\n",
        "            image batch has shape [B, K, N, 784] and label batch has shape [B, K, N, N]\n",
        "            where B is batch size, K is number of samples per class, N is number of classes\n",
        "        \"\"\"\n",
        "        if batch_type == \"train\":\n",
        "            folders = self.metatrain_character_folders\n",
        "        elif batch_type == \"val\":\n",
        "            folders = self.metaval_character_folders\n",
        "        else:\n",
        "            folders = self.metatest_character_folders\n",
        "        shuffle = batch_type == \"train\"\n",
        "        \n",
        "        # select classes for mini batch randomly\n",
        "        selected_classes = np.random.choice(folders, self.num_classes * batch_size)\n",
        "\n",
        "        # get image paths for mini batch\n",
        "        images_labels = get_images(selected_classes, [i for i in range(self.num_classes)] * batch_size, self.num_samples_per_class, False)\n",
        "\n",
        "        # split\n",
        "        all_label_batches, all_image_batches = zip(*images_labels)\n",
        "\n",
        "        # load paths as image array\n",
        "        all_image_batches = image_files_to_array(all_image_batches)\n",
        "        \n",
        "        # reshape\n",
        "        all_image_batches = all_image_batches.reshape(self.num_samples_per_class, batch_size, self.num_classes, self.dim_input)\n",
        "        all_image_batches = np.transpose(all_image_batches, (1, 0, 2, 3))\n",
        "        \n",
        "        # labels to one-hot encoded matrix\n",
        "        all_label_batches = np.eye(self.num_classes)[np.array(all_label_batches)]\n",
        "        all_label_batches = all_label_batches.reshape(self.num_samples_per_class, batch_size, self.num_classes, -1)\n",
        "        all_label_batches = np.transpose(all_label_batches, (1, 0, 2, 3))\n",
        "\n",
        "        # shuffle class order\n",
        "        random_idx = [np.random.permutation(self.num_classes) for _ in range(batch_size)]\n",
        "        all_image_batches, all_label_batches = zip(*[[i[:, :, r], l[:, :, r]] for i, l, r in zip(all_image_batches, all_label_batches, random_idx)])\n",
        "        \n",
        "        all_image_batches = np.stack(all_image_batches, axis=0)\n",
        "        all_label_batches = np.stack(all_label_batches, axis=0)\n",
        "\n",
        "        return all_image_batches.astype(np.float32), all_label_batches.astype(np.float32)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yvqTOqTHVA9"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class MANN(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, num_classes, samples_per_class):\n",
        "        super(MANN, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.samples_per_class = samples_per_class\n",
        "        self.layer1 = tf.keras.layers.LSTM(128, return_sequences=True)\n",
        "        self.layer2 = tf.keras.layers.LSTM(num_classes, return_sequences=True)\n",
        "\n",
        "    def call(self, input_images, input_labels):\n",
        "        \"\"\"\n",
        "        MANN\n",
        "        Args:\n",
        "            input_images: [B, K+1, N, 784] flattened images\n",
        "            labels: [B, K+1, N, N] ground truth labels\n",
        "        Returns:\n",
        "            [B, K+1, N, N] predictions\n",
        "        \"\"\"\n",
        "        #############################\n",
        "        #### YOUR CODE GOES HERE ####\n",
        "        B, K, N, C = input_images.shape\n",
        "        # reshape for rnn\n",
        "        input_images = tf.reshape(input_images, (B, -1, C))\n",
        "\n",
        "        # last sample(test) is filled with zero\n",
        "        input_labels = tf.concat([\n",
        "            input_labels[:, :-1], tf.zeros_like(input_labels[:, -1:])\n",
        "        ], axis=1)\n",
        "\n",
        "        # reshape for rnn\n",
        "        input_labels = tf.reshape(input_labels, (B, -1, N))\n",
        "\n",
        "        inputs = tf.concat([input_images, input_labels], axis=-1)\n",
        "        out = self.layer2(self.layer1(inputs))\n",
        "        out = tf.reshape(out, (-1, K, N, N))\n",
        "        #############################\n",
        "        return out\n",
        "\n",
        "    def loss_function(self, preds, labels):\n",
        "        \"\"\"\n",
        "        Computes MANN loss\n",
        "        Args:\n",
        "            preds: [B, K+1, N, N] network output\n",
        "            labels: [B, K+1, N, N] labels\n",
        "        Returns:\n",
        "            scalar loss\n",
        "        \"\"\"\n",
        "        #############################\n",
        "        #### YOUR CODE GOES HERE ####\n",
        "        preds = preds[:, -1:]\n",
        "        labels = labels[:, -1:]\n",
        "        losses = tf.nn.softmax_cross_entropy_with_logits(labels, preds)\n",
        "        return tf.math.reduce_mean(losses)\n",
        "        #############################\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels, model, optim, eval=False):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images, labels)\n",
        "        loss = model.loss_function(predictions, labels)\n",
        "    if not eval:\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return predictions, loss\n",
        "\n",
        "\n",
        "def main(num_classes=5, num_samples=1, meta_batch_size=16, random_seed=1234):\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    tf.random.set_seed(random_seed)\n",
        "\n",
        "    data_generator = DataGenerator(num_classes, num_samples + 1)\n",
        "\n",
        "    o = MANN(num_classes, num_samples + 1)\n",
        "    optim = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    for step in range(25000):\n",
        "        i, l = data_generator.sample_batch('train', meta_batch_size)\n",
        "        _, ls = train_step(i, l, o, optim)\n",
        "\n",
        "        if (step + 1) % 100 == 0:\n",
        "            print(\"*\" * 5 + \"Iter \" + str(step + 1) + \"*\" * 5)\n",
        "            i, l = data_generator.sample_batch('test', 100)\n",
        "            pred, tls = train_step(i, l, o, optim, eval=True)\n",
        "            print(\"Train Loss:\", ls.numpy(), \"Test Loss:\", tls.numpy())\n",
        "            pred = tf.reshape(pred, [-1, num_samples + 1, num_classes, num_classes])\n",
        "            pred = tf.math.argmax(pred[:, -1, :, :], axis=2)\n",
        "            l = tf.math.argmax(l[:, -1, :, :], axis=2)\n",
        "            print(\"Test Accuracy\", tf.reduce_mean(tf.cast(tf.math.equal(pred, l), tf.float32)).numpy())\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si10vH_0SH_y",
        "outputId": "ff2e3cc5-9d4e-45a0-a5d6-0f431dd50cf3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "results = main(num_classes=5, num_samples=1, meta_batch_size=16, random_seed=1234)\n",
        "#############################\n",
        "#### YOUR CODE GOES HERE ####\n",
        "# pass\n",
        "#############################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*****Iter 100*****\n",
            "Train Loss: 1.0228605 Test Loss: 0.9847459\n",
            "Test Accuracy 0.604\n",
            "*****Iter 200*****\n",
            "Train Loss: 0.80791837 Test Loss: 0.80565286\n",
            "Test Accuracy 0.766\n",
            "*****Iter 300*****\n",
            "Train Loss: 0.7241886 Test Loss: 0.70488244\n",
            "Test Accuracy 0.888\n",
            "*****Iter 400*****\n",
            "Train Loss: 0.6661855 Test Loss: 0.6631766\n",
            "Test Accuracy 0.948\n",
            "*****Iter 500*****\n",
            "Train Loss: 0.63941544 Test Loss: 0.6380839\n",
            "Test Accuracy 0.996\n",
            "*****Iter 600*****\n",
            "Train Loss: 0.6123941 Test Loss: 0.62419575\n",
            "Test Accuracy 0.986\n",
            "*****Iter 700*****\n",
            "Train Loss: 0.6145464 Test Loss: 0.6193505\n",
            "Test Accuracy 0.996\n",
            "*****Iter 800*****\n",
            "Train Loss: 0.61228603 Test Loss: 0.60849583\n",
            "Test Accuracy 1.0\n",
            "*****Iter 900*****\n",
            "Train Loss: 0.6016611 Test Loss: 0.60184896\n",
            "Test Accuracy 1.0\n",
            "*****Iter 1000*****\n",
            "Train Loss: 0.58975095 Test Loss: 0.5923217\n",
            "Test Accuracy 1.0\n",
            "*****Iter 1100*****\n",
            "Train Loss: 0.595824 Test Loss: 0.59046775\n",
            "Test Accuracy 1.0\n",
            "*****Iter 1200*****\n",
            "Train Loss: 0.57494533 Test Loss: 0.57899827\n",
            "Test Accuracy 0.998\n",
            "*****Iter 1300*****\n",
            "Train Loss: 0.56470644 Test Loss: 0.5686239\n",
            "Test Accuracy 1.0\n",
            "*****Iter 1400*****\n",
            "Train Loss: 0.5590906 Test Loss: 0.56081307\n",
            "Test Accuracy 1.0\n",
            "*****Iter 1500*****\n",
            "Train Loss: 0.5708366 Test Loss: 0.55408263\n",
            "Test Accuracy 1.0\n",
            "*****Iter 1600*****\n",
            "Train Loss: 0.5569105 Test Loss: 0.55955505\n",
            "Test Accuracy 1.0\n",
            "*****Iter 1700*****\n",
            "Train Loss: 0.61234605 Test Loss: 0.6143655\n",
            "Test Accuracy 0.97\n",
            "*****Iter 1800*****\n",
            "Train Loss: 0.59969777 Test Loss: 0.59252363\n",
            "Test Accuracy 0.99\n",
            "*****Iter 1900*****\n",
            "Train Loss: 0.5590825 Test Loss: 0.55280095\n",
            "Test Accuracy 1.0\n",
            "*****Iter 2000*****\n",
            "Train Loss: 0.5524491 Test Loss: 0.5513057\n",
            "Test Accuracy 1.0\n",
            "*****Iter 2100*****\n",
            "Train Loss: 0.5376578 Test Loss: 0.5431185\n",
            "Test Accuracy 1.0\n",
            "*****Iter 2200*****\n",
            "Train Loss: 0.54561776 Test Loss: 0.547828\n",
            "Test Accuracy 1.0\n",
            "*****Iter 2300*****\n",
            "Train Loss: 0.5560316 Test Loss: 0.5455566\n",
            "Test Accuracy 1.0\n",
            "*****Iter 2400*****\n",
            "Train Loss: 0.53402495 Test Loss: 0.5433682\n",
            "Test Accuracy 1.0\n",
            "*****Iter 2500*****\n",
            "Train Loss: 0.54235715 Test Loss: 0.5468425\n",
            "Test Accuracy 1.0\n",
            "*****Iter 2600*****\n",
            "Train Loss: 0.53202546 Test Loss: 0.54911923\n",
            "Test Accuracy 1.0\n",
            "*****Iter 2700*****\n",
            "Train Loss: 0.5399264 Test Loss: 0.5403308\n",
            "Test Accuracy 1.0\n",
            "*****Iter 2800*****\n",
            "Train Loss: 0.5332366 Test Loss: 0.54462415\n",
            "Test Accuracy 1.0\n",
            "*****Iter 2900*****\n",
            "Train Loss: 0.54827917 Test Loss: 0.5440191\n",
            "Test Accuracy 1.0\n",
            "*****Iter 3000*****\n",
            "Train Loss: 0.55242366 Test Loss: 0.5482466\n",
            "Test Accuracy 1.0\n",
            "*****Iter 3100*****\n",
            "Train Loss: 0.5426631 Test Loss: 0.54108757\n",
            "Test Accuracy 1.0\n",
            "*****Iter 3200*****\n",
            "Train Loss: 0.5564129 Test Loss: 0.567892\n",
            "Test Accuracy 0.988\n",
            "*****Iter 3300*****\n",
            "Train Loss: 0.5257552 Test Loss: 0.5340662\n",
            "Test Accuracy 1.0\n",
            "*****Iter 3400*****\n",
            "Train Loss: 0.5218797 Test Loss: 0.5279647\n",
            "Test Accuracy 1.0\n",
            "*****Iter 3500*****\n",
            "Train Loss: 0.52341616 Test Loss: 0.5239529\n",
            "Test Accuracy 1.0\n",
            "*****Iter 3600*****\n",
            "Train Loss: 0.5391328 Test Loss: 0.5237004\n",
            "Test Accuracy 1.0\n",
            "*****Iter 3700*****\n",
            "Train Loss: 0.5201391 Test Loss: 0.5244461\n",
            "Test Accuracy 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox2yVAuKQkLE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}